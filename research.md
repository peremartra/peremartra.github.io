---
layout: page
title: Research & Writing
permalink: /research/
---

## Books

### Rearchitecting LLMs *(Manning Publications, forthcoming)*
A comprehensive guide to optimizing large language models for production deployment. Topics include structured pruning, knowledge distillation, fairness-aware optimization, and practical strategies for model efficiency.

**Status**: In progress | **Expected**: 2026  
**MEAP Launch**: Q4 2025 / Q1 2026

### Large Language Models Projects *(Apress, 2024)*
Practical guide to building real-world applications with large language models. Covers implementation strategies, best practices, and production considerations.

[View on Apress](https://www.apress.com/gp/book/9781484296622)

---

## Open Source Projects

### [OptiPfair](https://github.com/peremartra/optipfair)
Fairness-aware structured pruning for language models. Implements novel techniques for optimizing LLMs while maintaining or improving fairness metrics across demographic groups.

**Key Features**:
- Differential activation analysis
- Width pruning in MLP expansion layers
- Comprehensive bias benchmarking (MMLU, GSM8K, BBQ, EsBBQ)

### [WizardSData](https://github.com/peremartra/wizardsdata)
Data processing and curation toolkit for LLM training and fine-tuning.

**Combined GitHub Activity**: 1,700+ stars

---

## Research Focus

### Current Areas
- **Structured Pruning**: GLU expansion ratios, attention bypass techniques
- **Fairness Optimization**: Bias detection and mitigation in pruned models
- **Adaptive Inference**: Dynamic model selection and routing
- **Agent Migration**: Frameworks for transitioning from proprietary to OSS models

### Published Work
- GLU expansion ratios and structured pruning techniques
- Fairness-aware optimization in resource-constrained LLMs
- Contributions to Hugging Face documentation and examples
- Google Gemini Cookbooks contributions

---

## Speaking

**STAC Summit 2025** - Presented on LLM optimization strategies for production deployment

---

[← Back to Home](/) | [Training Programs →](/training)
